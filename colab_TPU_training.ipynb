{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pedalnet_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssgalitsky/PedalNetRT/blob/master/colab_TPU_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-9U7OducSMH"
      },
      "source": [
        "PyTorch Lightning Documentation https://pytorch-lightning.readthedocs.io/en/stable/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpiGhaArUL6H",
        "cellView": "form",
        "outputId": "d3d9aa46-a120-49fe-b668-1645fad0ed46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#@title 1. clone  https://github.com/ssgalitsky/PedalNetRT.git\n",
        "!git clone https://github.com/ssgalitsky/PedalNetRT.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PedalNetRT'...\n",
            "remote: Enumerating objects: 151, done.\u001b[K\n",
            "remote: Counting objects: 100% (151/151), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 151 (delta 68), reused 27 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (151/151), 56.20 MiB | 3.21 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHkf8SbECuIn",
        "cellView": "both"
      },
      "source": [
        "#@title 2. install dependencies (TPU)\n",
        "!pip3 install torchvision\n",
        "!pip3 install pytorch_lightning==0.9.0\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python3 pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "!pip3 install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl\n",
        "\n",
        "import torch\n",
        "use_cuda=True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  net.cuda()\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/PedalNetRT')\n",
        "#os.getcwd()\n",
        "\n",
        "#!ffmpeg -i \"data/flute.wav\"\n",
        "#!ffmpeg -i \"data/guitar.wav\"\n",
        "#!ffmpeg -i \"data/guitar.wav\" -acodec pcm_f32le -ar 44100 -ac 1 \"data/guitar.wav\"\n",
        "#!ffmpeg -i \"data/flute.wav\" -acodec pcm_f32le -ar 44100 -ac 1 \"data/flute.wav\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPznmepVwpwT"
      },
      "source": [
        "#@title 2. install dependencies (GPU)\n",
        "!pip3 install torchvision\n",
        "!pip3 install pytorch_lightning==0.9.0\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python3 pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "#!pip3 install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl\n",
        "\n",
        "import torch\n",
        "use_cuda=True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  net.cuda()\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/PedalNetRT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9vktclZBGuc",
        "cellView": "both",
        "outputId": "038e603b-d600-4004-ef25-6acd0e535c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#@title 3. prepare data\n",
        "!python3 \"prepare_data.py\" \"/content/PedalNetRT/data/guitar.wav\" \"/content/PedalNetRT/data/voice.wav\"\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prepare_data.py:8: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  in_rate, in_data = wavfile.read(args.in_file)\n",
            "prepare_data.py:9: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  out_rate, out_data = wavfile.read(args.out_file)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBb2THG2BtLm",
        "cellView": "both"
      },
      "source": [
        "#@title 4. **train model**\n",
        "import os\n",
        "#assert os.environ['COLAB_TPU_ADDR']\n",
        "import pytorch_lightning\n",
        "#!python3 \"train.py\" --batch_size=32 --max_epochs=1500 --learning_rate=3e-3 --num_channels=5 \n",
        "!python3 \"train.py\" --batch_size=32 --max_epochs=100 --learning_rate=3e-3 --num_channels=5 \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTE8SkYlaoB5",
        "cellView": "form"
      },
      "source": [
        "#@title 4. train.py\n",
        "import pytorch_lightning as pl\n",
        "from model import PedalNet\n",
        "\n",
        "\n",
        "def trainFit(args):\n",
        "    model = PedalNet(args)\n",
        "    trainer = pl.Trainer(\n",
        "        #max_epochs=args.max_epochs, gpus=args.gpus, row_log_interval=100\n",
        "        # The following line is for use with the Colab notebook when training on TPUs.\n",
        "        # Comment out the above line and uncomment the below line to use.\n",
        "        \n",
        "         max_epochs=args.max_epochs, tpu_cores=args.tpu_cores, row_log_interval=100\n",
        "    )\n",
        "    trainer.fit(model)\n",
        "\n",
        "    parser.add_argument(\"--num_channels\", type=int, default=5)\n",
        "    parser.add_argument(\"--dilation_depth\", type=int, default=10)\n",
        "    parser.add_argument(\"--num_repeat\", type=int, default=1)\n",
        "    \n",
        "    # filter_width=kernel_size\n",
        "    parser.add_argument(\"--kernel_size\", type=int, default=3)\n",
        "\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=3e-3)\n",
        "\n",
        "    parser.add_argument(\"--max_epochs\", type=int, default=100)\n",
        "    parser.add_argument(\"--gpus\", default=\"0\")\n",
        "    parser.add_argument(\"--tpu_cores\", default=\"8\")\n",
        "\n",
        "    parser.add_argument(\"--data\", default=\"data.pickle\")\n",
        "    args = parser.parse_args()\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7W1bPk7V0hp",
        "cellView": "form"
      },
      "source": [
        "#@title 5. convert torch model (ckpt) to plugin model (json) \n",
        "# The .ckpt model must be converted to a .json model to run in the plugin. Usage:\n",
        "\n",
        "#!python3 convert_pedalnet_to_wavnetva.py --model=your_trained_model.ckpt\n",
        "!python3 convert_pedalnet_to_wavnetva.py --model=your_trained_model.ckpt\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jinHl027kvhC",
        "cellView": "form"
      },
      "source": [
        "#@title 6. convert model from TPU to CPU format\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "from model import PedalNet\n",
        "# Change path below to match model file\n",
        "model = PedalNet.load_from_checkpoint('lightning_logs/version_0/checkpoints/epoch=1.ckpt')\n",
        "\n",
        "# xm.save(model.state_dict(), 'tpu_to_cpu.ckpt') \n",
        "xm.save(model, 'tpu_to_cpu_model.ckpt') \n",
        "# issues loading model, see pytorch_lightning issues #2303 and #3044 (Might be completed by now)\n",
        "\n",
        "# TODO: Add code to be able to load saved model sucessfully\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}